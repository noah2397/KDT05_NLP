{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Happy', ',', 'New', 'Year', '!', 'Do', \"n't\", 'stop', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text=\"Happy, New Year! Don't stop. \"\n",
    "result = word_tokenize(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Happy', ',', 'New', 'Year', '!', 'Don', \"'\", 't', 'stop', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "text=\"Happy, New Year! Don't stop. \"\n",
    "wp_tokenizer = WordPunctTokenizer()\n",
    "result = wp_tokenizer.tokenize(text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "text = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "You can see it out your window or on your television. \\\n",
    "You feel it when you go to work, or go to church or pay your taxes.'\n",
    "result = sent_tokenize(text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "text = 'I am actively looking for Ph.D. students. and you are a Ph.D student.'\n",
    "result = sent_tokenize(text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " was wondering anyone out there could enlighten this car.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"I was wondering if anyone out there could enlighten me on this car.\"\n",
    "# 길이가 1~2인 단어들 정규 표현식 이용하여 삭제\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "print(shortword.sub('', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 : 179\n",
      "불용어 20개 출력 : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords # 불용어 패키지 다운로드\n",
    "from nltk.tokenize import word_tokenize \n",
    "stop_words_list = stopwords.words( 'english' )\n",
    "print('불용어 개수 :', len(stop_words_list))\n",
    "print('불용어 20개 출력 :',stop_words_list[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거 전 : ['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.'], 불용어 제거 후 : ['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
     ]
    }
   ],
   "source": [
    "example = \"Family is not an important thing. It's everything.\"\n",
    "stop_words = set(stopwords.words('english')) \n",
    "word_tokens = word_tokenize(example)\n",
    "result = []\n",
    "for word in word_tokens: \n",
    "    if word not in stop_words: result.append(word) \n",
    "print(f'불용어 제거 전 : {word_tokens}, 불용어 제거 후 : {result}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시소러스(어휘분류사전, Thesaurus)\n",
    "- 단어 의미의 계층적 구조를 분석, 분류 후 구축된 데이터베이스\n",
    "영어 어휘목록/거대한 Database - WordNet\n",
    "• 프린스턴 대학 심리학 교수 조지 A. 밀러의 인지 과학 연구소에서 개발\n",
    "• 유의어 집단으로 분류하고 간략하고 일반적인 정의 제공\n",
    "• 어휘목록 사이의 다양한 의미 관계 기록\n",
    "• 목적\n",
    "  사전(단어집)과 시소러스(유의어·반의어 사전) 구성으로 직관적으로 사용 가능\n",
    "  자동화된 본문 분석과 인공 지능 응용 뒷받침\n",
    "\n",
    "• 전치사, 관사, 대명사 같은 기능어들 제외\n",
    "• 명사, 동사, 형용사, 부사 정보만 제공\n",
    "• Synset 개념 : 동의어 또는 의미적 동일한 데이터 그룹\n",
    "    • synonym : 동의어\n",
    "    • antonym : 반의어\n",
    "    • hypernym : 상의어\n",
    "    • hyponym : 하위어\n",
    "    • coordinate term : 동의어\n",
    "    • holonym : 전체어\n",
    "    • meronym : 부분어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 객체 : Synset('cookbook.n.01')\n",
      "객체 이름 : cookbook.n.01\n",
      "단어 추출 : cookbook\n",
      "단어 정의 : a book of recipes and cooking directions\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syn = wordnet.synsets('cookbook')[0] # 단어 유의어 집단 객체 추출\n",
    "print(f'단어 객체 : {syn}') # 객체 이름\n",
    "print(f'객체 이름 : {syn.name()}') # 객체 이름\n",
    "print(f'단어 추출 : {syn.lemmas()[0].name()}') # 단어만 표시\n",
    "print(f'단어 정의 : {syn.definition()}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "상의어 : [Synset('reference_book.n.01')]\n",
      "하의어 : [Synset('annual.n.02'), Synset('atlas.n.02'), Synset('cookbook.n.01'), Synset('directory.n.01'), Synset('encyclopedia.n.01'), Synset('handbook.n.01'), Synset('instruction_book.n.01'), Synset('source_book.n.01'), Synset('wordbook.n.01')]\n",
      "루트 상의어 : [Synset('entity.n.01')]\n"
     ]
    }
   ],
   "source": [
    "# 상의어(hypernym)\n",
    "print(f'상의어 : {syn.hypernyms()}')\n",
    "print(f'하의어 : {syn.hypernyms()[0].hyponyms()}')\n",
    "print(f'루트 상의어 : {syn.root_hypernyms()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cook.v.01 단어 원형 : [Lemma('cook.v.01.cook')]\n"
     ]
    }
   ],
   "source": [
    "# 어근 추출\n",
    "syn = wordnet.synsets('cooked')[0]\n",
    "print(f'{syn.name()} 단어 원형 : {syn.lemmas()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('cookbook.n.01')와 Synset('instruction_book.n.01') 유사도 : 0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "# 상의어(hypernym)트리 구성 ==> 단어간의 유사도(similarity) 계산에 사용\n",
    "cb = wordnet.synset('cookbook.n.01')\n",
    "ib = wordnet.synset('instruction_book.n.01')\n",
    "print(f'{cb}와 {ib} 유사도 : {cb.wup_similarity(ib)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n",
      "fant fanciest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer \n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print( stemmer.stem('working'), stemmer.stem('works') ,stemmer.stem('worked'))\n",
    "print( stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\n",
    "print( stemmer.stem('happier'), stemmer.stem('happiest') )\n",
    "print( stemmer.stem('fancier'), stemmer.stem('fanciest') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "happy happy\n",
      "fancy fancy\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "lemma = WordNetLemmatizer( ) ## nltk.download('wordnet')\n",
    "print(lemma.lemmatize('amusing','v'),lemma.lemmatize('amuses','v'),lemma.lemmatize('amused','v'))\n",
    "print(lemma.lemmatize('happier','a'),lemma.lemmatize('happiest','a'))\n",
    "print(lemma.lemmatize('fancier','a'),lemma.lemmatize('fanciest','a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "포터 어간 추출 후: ['i', 'am', 'the going to th', 'have']\n",
      "랭커스터 어간 추출 후: ['i', 'am', 'the going to th', 'hav']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer # 정밀하게 설계되어 정확도 높음, 영어 어간 추출에 좋은 성능\n",
    "from nltk.stem import LancasterStemmer # Porter의 약 2배인 100개 이상의 규칙 \n",
    "\n",
    "words=[\"I\",\"am\", \"the going to the\" ,\"having\"]\n",
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "print('포터 어간 추출 후:',[porter_stemmer.stem(w) for w in words])\n",
    "print('랭커스터 어간 추출 후:',[lancaster_stemmer.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "표제어 추출 전 :['policy', 'the going', 'am', 'doing', 'organization', 'having', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
      "표제어 추출 후 :['policy', 'the going', 'am', 'doing', 'organization', 'having', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer # 표제어 추출 => 원형만을 뽑음 \n",
    "words = ['policy', 'the going', 'am', 'doing', 'organization', 'having', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "result = [ lemmatizer.lemmatize(word) for word in words ]\n",
    "print(f'표제어 추출 전 :{words}')\n",
    "print(f'표제어 추출 후 :{result}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fret1=> None \n",
      "ret1=> <re.Match object; span=(0, 3), match='abc'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "r = re.compile(\"a.c\") # a와 c 사이 어떤 1개 문자\n",
    "ret1=r.search(\"kkk\") \n",
    "ret2=r.search('abc')\n",
    "print(f'fret1=> {ret1} \\nret1=> {ret2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fret1=> None \n",
      "ret1=> <re.Match object; span=(0, 3), match='abc'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "r = re.compile(\"a.c\") # a와 c 사이 어떤 1개 문자\n",
    "ret1=r.match (\"kkka4b\") \n",
    "ret2=r.match('abc')\n",
    "print(f'fret1=> {ret1} \\nret1=> {ret2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ret2:<re.Match object; span=(0, 4), match='abab'>\n",
      "abab\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "p = re.compile('[a-z]+')\n",
    "ret2=p.match(\"abab\")\n",
    "if ret2:\n",
    "    print(f'ret2:{ret2}\\n{ ret2.group()}') #group() 매치된 문자열 리턴\n",
    "else:\n",
    "    print('No Match')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'Mr', 'Jone', 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n",
      "[\"Don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name,', 'Mr.', \"Jone's\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "text = \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop\"\n",
    "tokenizer1 = RegexpTokenizer(\"[\\w]+\")\n",
    "tokenizer2 = RegexpTokenizer(\"\\s+\", gaps=True)\n",
    "print(tokenizer1.tokenize(text))\n",
    "print(tokenizer2.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
