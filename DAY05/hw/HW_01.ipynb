{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 목표 : torchtext 라이브러리로 텍스트 분류\n",
    "# 데이터 준비\n",
    "import torch\n",
    "from torchtext.datasets import AG_NEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "train_iter, test_iter = AG_NEWS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text) \n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\",\"<pad>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 > 정수 인코딩\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "\n",
    "# 레이블 > 정수 인코딩\n",
    "label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list, text_list, offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchtext.vocab.vocab.Vocab"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0, Token: <unk>\n"
     ]
    }
   ],
   "source": [
    "# https://pytorch.org/text/stable/vocab.html\n",
    "# 역시 문서 찾는게 AI보다 낫다....\n",
    "for index, token in enumerate(vocab.get_itos()):\n",
    "    print(f\"Index: {index}, Token: {token}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_id = {token: idx for idx, token in enumerate(vocab.get_itos())}\n",
    "id_to_token = {idx: token for idx, token in enumerate(vocab.get_itos())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\TEXT_017_220_38\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\combining.py:337: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 6,\n",
       " 1868,\n",
       " 1868,\n",
       " 0,\n",
       " 0,\n",
       " 84,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 2884,\n",
       " 6,\n",
       " 1995,\n",
       " 10,\n",
       " 0,\n",
       " 0,\n",
       " 1868,\n",
       " 6,\n",
       " 1091,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 987,\n",
       " 3369,\n",
       " 0,\n",
       " 0,\n",
       " 138,\n",
       " 84,\n",
       " 372,\n",
       " 0,\n",
       " 84,\n",
       " 2960,\n",
       " 2884,\n",
       " 0,\n",
       " 0,\n",
       " 1868,\n",
       " 6,\n",
       " 987,\n",
       " 3369,\n",
       " 0,\n",
       " 14,\n",
       " 0,\n",
       " 2884,\n",
       " 52,\n",
       " 84,\n",
       " 2884,\n",
       " 1995,\n",
       " 10,\n",
       " 15,\n",
       " 0,\n",
       " 0,\n",
       " 2884,\n",
       " 52,\n",
       " 84,\n",
       " 2884,\n",
       " 1995,\n",
       " 10,\n",
       " 0,\n",
       " 16,\n",
       " 0,\n",
       " 0,\n",
       " 2960,\n",
       " 372,\n",
       " 1995,\n",
       " 84,\n",
       " 16,\n",
       " 10,\n",
       " 2884,\n",
       " 1868,\n",
       " 1868,\n",
       " 2884,\n",
       " 1995,\n",
       " 10,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 1868,\n",
       " 1868,\n",
       " 0,\n",
       " 0,\n",
       " 84,\n",
       " 1995,\n",
       " 2884,\n",
       " 2884,\n",
       " 84,\n",
       " 17,\n",
       " 10,\n",
       " 0,\n",
       " 902,\n",
       " 1091,\n",
       " 283,\n",
       " 138,\n",
       " 902,\n",
       " 1868,\n",
       " 283,\n",
       " 138,\n",
       " 3366,\n",
       " 1705,\n",
       " 2207,\n",
       " 6,\n",
       " 138,\n",
       " 902,\n",
       " 0,\n",
       " 372,\n",
       " 2339,\n",
       " 0,\n",
       " 52,\n",
       " 1868,\n",
       " 84,\n",
       " 1995,\n",
       " 6,\n",
       " 16,\n",
       " 987,\n",
       " 5001,\n",
       " 138,\n",
       " 283,\n",
       " 987,\n",
       " 10,\n",
       " 4,\n",
       " 0,\n",
       " 6,\n",
       " 1995,\n",
       " 2884,\n",
       " 0,\n",
       " 10,\n",
       " 2884,\n",
       " 2884,\n",
       " 283,\n",
       " 138,\n",
       " 3366,\n",
       " 0,\n",
       " 3366,\n",
       " 1995,\n",
       " 2884,\n",
       " 2884,\n",
       " 138,\n",
       " 0,\n",
       " 6,\n",
       " 3366,\n",
       " 6,\n",
       " 283,\n",
       " 138,\n",
       " 2]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pad_sequences(sequences, max_length, pad_value, start=\"R\"):\n",
    "    result = list()\n",
    "    for sequence in sequences:\n",
    "        sequence = sequence[:max_length] if start == \"R\" else sequence[-1*max_length:]\n",
    "        pad_length = max_length - len(sequence)\n",
    "        padded_sequence = sequence + [pad_value] * pad_length if start == \"R\" else  [pad_value] * pad_length + sequence\n",
    "        result.append(padded_sequence)\n",
    "    return np.asarray(result)\n",
    "\n",
    "unk_id = token_to_id[\"<unk>\"]\n",
    "\n",
    "train_ids = [\n",
    "    [token_to_id.get(token, unk_id) for token in review] for _, review in train_iter\n",
    "]\n",
    "test_ids = [\n",
    "    [token_to_id.get(token, unk_id) for token in review] for _, review in test_iter\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    6 1868 1868    0    0   84    2    0    0 2884    6 1995   10\n",
      "    0    0 1868    6 1091    0    0    6  987 3369    0    0  138   84\n",
      "  372    0   84 2960]\n",
      "[   0 2884    6 1995   10    0 2339  372 1995    0    0    0    0    0\n",
      "  954 2884  138   10  283  372  138    0    6 2339   84 2884 1995    0\n",
      "   84    6 1868 3369]\n"
     ]
    }
   ],
   "source": [
    "max_length = 32\n",
    "pad_id = token_to_id[\"<pad>\"]\n",
    "\n",
    "train_ids = pad_sequences(train_ids, max_length, pad_id, start=\"R\")\n",
    "test_ids = pad_sequences(test_ids, max_length, pad_id, start=\"R\")\n",
    "\n",
    "print(train_ids[0])\n",
    "print(test_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mathn\\AppData\\Local\\Temp\\ipykernel_19148\\2660737245.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_ids = torch.tensor(train_ids)\n",
      "C:\\Users\\mathn\\AppData\\Local\\Temp\\ipykernel_19148\\2660737245.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_ids = torch.tensor(test_ids)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_ids = torch.tensor(train_ids)\n",
    "test_ids = torch.tensor(test_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_labels = torch.tensor([label-1 for label, _ in train_iter], dtype=torch.float32)\n",
    "test_labels = torch.tensor([label-1 for label, _ in test_iter], dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(train_ids, train_labels)\n",
    "test_dataset = TensorDataset(test_ids, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_vocab,\n",
    "        hidden_dim,\n",
    "        embedding_dim,\n",
    "        n_layers,\n",
    "        dropout=0.5,\n",
    "        bidirectional=True,\n",
    "        model_type=\"lstm\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=0\n",
    "        )\n",
    "        if model_type == \"rnn\":\n",
    "            self.model = nn.RNN(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "            )\n",
    "        elif model_type == \"lstm\":\n",
    "            self.model = nn.LSTM(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "            )\n",
    "\n",
    "        if bidirectional:\n",
    "            self.classifier = nn.Linear(hidden_dim * 2, 4)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(hidden_dim, 4)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        output, _ = self.model(embeddings)\n",
    "        last_output = output[:, -1, :]\n",
    "        last_output = self.dropout(last_output)\n",
    "        logits = self.classifier(last_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "\n",
    "n_vocab = len(token_to_id)\n",
    "hidden_dim = 64 \n",
    "embedding_dim = 128\n",
    "n_layers = 2\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "classifier = SentenceClassifier( \n",
    "    n_vocab=n_vocab, \n",
    "    hidden_dim=hidden_dim, \n",
    "    embedding_dim=embedding_dim, \n",
    "    n_layers=n_layers\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.AdamW(classifier.parameters(), lr=0.001)  # 학습률이 빠른 AdamW로 옵티마이저 설정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0 : 1.3392443656921387, Train Accuracy : 0.4375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 500 : 1.3809785226623932, Train Accuracy : 0.28642714570858285\n",
      "Train Loss 1000 : 1.379379356300438, Train Accuracy : 0.29164585414585414\n",
      "Train Loss 1500 : 1.3706254418256838, Train Accuracy : 0.30654563624250497\n",
      "Train Loss 2000 : 1.3597825100158585, Train Accuracy : 0.3241816591704148\n",
      "Train Loss 2500 : 1.346627144802098, Train Accuracy : 0.3398890443822471\n",
      "Train Loss 3000 : 1.3327666244003147, Train Accuracy : 0.3551316227924025\n",
      "Train Loss 3500 : 1.3192014002146226, Train Accuracy : 0.3680734075978292\n",
      "Train Loss 4000 : 1.3079322463898204, Train Accuracy : 0.37940514871282177\n",
      "Train Loss 4500 : 1.295165769419811, Train Accuracy : 0.39130193290379917\n",
      "Train Loss 5000 : 1.2826194867709235, Train Accuracy : 0.40293191361727654\n",
      "Train Loss 5500 : 1.271259783636633, Train Accuracy : 0.4124704599163788\n",
      "Train Loss 6000 : 1.2588925342562993, Train Accuracy : 0.42210673221129813\n",
      "Train Loss 6500 : 1.246810514144651, Train Accuracy : 0.43127018920166127\n",
      "Train Loss 7000 : 1.2349359751547972, Train Accuracy : 0.44000857020425654\n",
      "Train Loss 0 : 1.0196925401687622, Train Accuracy : 0.5625\n",
      "Train Loss 500 : 1.0379460433761993, Train Accuracy : 0.5800898203592815\n",
      "Train Loss 1000 : 1.0283821285306871, Train Accuracy : 0.5846028971028971\n",
      "Train Loss 1500 : 1.0155893254089483, Train Accuracy : 0.5886075949367089\n",
      "Train Loss 2000 : 1.0127090286726002, Train Accuracy : 0.5901736631684158\n",
      "Train Loss 2500 : 1.0084620119571113, Train Accuracy : 0.5907886845261895\n",
      "Train Loss 3000 : 1.0012894045686929, Train Accuracy : 0.5943435521492836\n",
      "Train Loss 3500 : 0.9955796729459384, Train Accuracy : 0.5980077120822622\n",
      "Train Loss 4000 : 0.9912393026070665, Train Accuracy : 0.6007560609847538\n",
      "Train Loss 4500 : 0.986266938474967, Train Accuracy : 0.6035047767162852\n",
      "Train Loss 5000 : 0.980123664582641, Train Accuracy : 0.6069536092781443\n",
      "Train Loss 5500 : 0.9758298893968228, Train Accuracy : 0.6084575531721506\n",
      "Train Loss 6000 : 0.9716973032202051, Train Accuracy : 0.6108356940509915\n",
      "Train Loss 6500 : 0.9676935734005823, Train Accuracy : 0.6129153207198893\n",
      "Train Loss 7000 : 0.9633082208540112, Train Accuracy : 0.6150907013283816\n",
      "Train Loss 0 : 0.6968469619750977, Train Accuracy : 0.6875\n",
      "Train Loss 500 : 0.8724361403497631, Train Accuracy : 0.6586826347305389\n",
      "Train Loss 1000 : 0.87085973058309, Train Accuracy : 0.660526973026973\n",
      "Train Loss 1500 : 0.8654139936168856, Train Accuracy : 0.6633494337108594\n",
      "Train Loss 2000 : 0.8609827545539908, Train Accuracy : 0.6655422288855573\n",
      "Train Loss 2500 : 0.8596472454304601, Train Accuracy : 0.6660085965613755\n",
      "Train Loss 3000 : 0.8569325537373328, Train Accuracy : 0.6679440186604465\n",
      "Train Loss 3500 : 0.8560570481503837, Train Accuracy : 0.6680948300485575\n",
      "Train Loss 4000 : 0.854402224288616, Train Accuracy : 0.6689733816545863\n",
      "Train Loss 4500 : 0.8518538745548481, Train Accuracy : 0.6706981781826261\n",
      "Train Loss 5000 : 0.8504865437364655, Train Accuracy : 0.6713032393521295\n",
      "Train Loss 5500 : 0.8488362315995458, Train Accuracy : 0.6725027267769497\n",
      "Train Loss 6000 : 0.8455787403491949, Train Accuracy : 0.674335527412098\n",
      "Train Loss 6500 : 0.8428912417070075, Train Accuracy : 0.675703737886479\n",
      "Train Loss 7000 : 0.8413948073819303, Train Accuracy : 0.6764390801314099\n",
      "Train Loss 0 : 0.6466355323791504, Train Accuracy : 0.75\n",
      "Train Loss 500 : 0.793183839368725, Train Accuracy : 0.6986027944111777\n",
      "Train Loss 1000 : 0.7910312050944204, Train Accuracy : 0.7032342657342657\n",
      "Train Loss 1500 : 0.7861268340469122, Train Accuracy : 0.7046552298467689\n",
      "Train Loss 2000 : 0.786747145524685, Train Accuracy : 0.7018990504747626\n",
      "Train Loss 2500 : 0.7835894661133692, Train Accuracy : 0.7033686525389844\n",
      "Train Loss 3000 : 0.7821101853943634, Train Accuracy : 0.7040361546151283\n",
      "Train Loss 3500 : 0.7801814054683494, Train Accuracy : 0.7049950014281634\n",
      "Train Loss 4000 : 0.7788282563591921, Train Accuracy : 0.7059328917770558\n",
      "Train Loss 4500 : 0.7782216842069649, Train Accuracy : 0.7061902910464342\n",
      "Train Loss 5000 : 0.7773822332067362, Train Accuracy : 0.7063837232553489\n",
      "Train Loss 5500 : 0.7767204634331938, Train Accuracy : 0.7070305399018361\n",
      "Train Loss 6000 : 0.7766298049093385, Train Accuracy : 0.7070488251958007\n",
      "Train Loss 6500 : 0.7761525109197117, Train Accuracy : 0.7070258421781265\n",
      "Train Loss 7000 : 0.7750903884992653, Train Accuracy : 0.7075774889301528\n",
      "Train Loss 0 : 0.4993268847465515, Train Accuracy : 0.8125\n",
      "Train Loss 500 : 0.7409692193695647, Train Accuracy : 0.7239271457085829\n",
      "Train Loss 1000 : 0.7365677636730802, Train Accuracy : 0.7266483516483516\n",
      "Train Loss 1500 : 0.7371192553653311, Train Accuracy : 0.7243920719520319\n",
      "Train Loss 2000 : 0.7384706532162824, Train Accuracy : 0.7246376811594203\n",
      "Train Loss 2500 : 0.7363337621074922, Train Accuracy : 0.7253098760495802\n",
      "Train Loss 3000 : 0.7372581354481742, Train Accuracy : 0.724737587470843\n",
      "Train Loss 3500 : 0.7332629681706122, Train Accuracy : 0.725524850042845\n",
      "Train Loss 4000 : 0.7329609312547263, Train Accuracy : 0.7261153461634592\n",
      "Train Loss 4500 : 0.7311563097458632, Train Accuracy : 0.7271161964007998\n",
      "Train Loss 5000 : 0.7298982535313664, Train Accuracy : 0.7276544691061788\n",
      "Train Loss 5500 : 0.728720679318465, Train Accuracy : 0.7280948918378477\n",
      "Train Loss 6000 : 0.728425956921088, Train Accuracy : 0.7286077320446592\n",
      "Train Loss 6500 : 0.7288007991389299, Train Accuracy : 0.7286186740501461\n",
      "Train Loss 7000 : 0.7292420038996791, Train Accuracy : 0.7284762891015569\n"
     ]
    }
   ],
   "source": [
    "def train(model, datasets, criterion, optimizer, device, interval):\n",
    "    model.train()\n",
    "    losses = list()\n",
    "    corrects = 0\n",
    "    total = 0\n",
    "\n",
    "    for step, (input_ids, labels) in enumerate(datasets): \n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels.long())\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        corrects += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        if step % interval == 0:\n",
    "            accuracy = corrects / total\n",
    "            print(f\"Train Loss {step} : {np.mean(losses)}, Train Accuracy : {accuracy}\")\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "interval = 500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(classifier, train_loader, criterion, optimizer, device, interval)\n",
    "# 힘내라 내 모델아.... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 55m 30.7s...많이도 걸렸다!\n",
    "# 근데 빅분기 과락나왔다...큰일났다 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TEXT_017_220_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
