{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(\"PP.py\"))))\n",
    "\n",
    "from PP import NLP_PreProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ê¸°ë°˜ í† í¬ë‚˜ì´ì € : Soynlp\n",
    "# í’ˆì‚¬ íƒœê¹…, ë‹¨ì–´ í† í°í™” ë“±ì„ ì§€ì›í•˜ëŠ” ë‹¨ì–´ í† í¬ë‚˜ì´ì €\n",
    "# ë¹„ì§€ë„ í•™ìŠµìœ¼ë¡œ ë‹¨ì–´ í† í°í™” : ë°ì´í„°ì— ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ë“¤ì„ ë‹¨ì–´ë¡œ ë¶„ì„\n",
    "# ë‚´ë¶€ì ìœ¼ë¡œ ë‹¨ì–´ ì ìˆ˜ í‘œë¡œ ë™ì‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install soynlp\n",
    "# https://github.com/lovit/soynlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ì—ì´', 'ë¹„ì‹ìŠ¤', 'ì´ëŒ€', 'íœ˜', '1ì›”', 'ìµœì• ', 'ëŒ', 'ê¸°ë¶€', 'ìš”ì •']\n",
      "['ì—ì´', 'ë¹„ì‹ìŠ¤', 'ì´ëŒ€', 'íœ˜', '1ì›”', 'ìµœì• ', 'ëŒ', 'ê¸°ë¶€', 'ìš”ì •', 'ì´ë‹¤']\n",
      "['ì—ì´', 'ë¹„ì‹ìŠ¤', 'ì´ëŒ€', 'íœ˜', '1ì›”', 'ìµœì• ', 'ëŒ', 'ê¸°ë¶€', 'ìš”ì •']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "tokenizer = Okt()\n",
    "\n",
    "print(tokenizer.morphs('ì—ì´ë¹„ì‹ìŠ¤ ì´ëŒ€íœ˜ 1ì›” ìµœì• ëŒ ê¸°ë¶€ ìš”ì •'))\n",
    "\n",
    "## í˜•íƒœì†Œ ë¶„ì„ ì‹œ ë§¤ê°œë³€ìˆ˜ stem=True ì„¤ì •\n",
    "print(tokenizer.morphs('ì—ì´ë¹„ì‹ìŠ¤ ì´ëŒ€íœ˜ 1ì›” ìµœì• ëŒ ê¸°ë¶€ ìš”ì • ì…ë‹ˆë‹¤', stem=True))\n",
    "\n",
    "print(tokenizer.morphs(\"ì—ì´ë¹„ì‹ìŠ¤ ì´ëŒ€íœ˜ 1ì›” ìµœì• ëŒ ê¸°ë¶€ ìš”ì •\", norm=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Sonlpy]ì‚¬ìš© => ë§ë­‰ì¹˜ ë°ì´í„°ì…‹ìœ¼ë¡œ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "filename = \"text_data.txt\"\n",
    "\n",
    "if not os.path.exists(path=filename):\n",
    "    urlretrieve(\"https://raw.githubusercontent.com/lovit/soynlp/master/tutorials/2016-10-20.txt\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "## í•™ìŠµ ë°ì´í„° ì²˜ë¦¬\n",
    "from soynlp import DoublespaceLineCorpus\n",
    "from soynlp.word import WordExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í›ˆë ¨ ë°ì´í„° ë¬¸ì„œ : 30091ê°œ\n"
     ]
    }
   ],
   "source": [
    "## í›ˆë ¨ ë°ì´í„° ë¬¸ì„œ ë¶„ë¦¬\n",
    "corpus = DoublespaceLineCorpus(filename)\n",
    "print(f\"í›ˆë ¨ ë°ì´í„° ë¬¸ì„œ : {len(corpus)}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.823 Gb\n",
      "all cohesion probabilities was computed. # words = 223348\n",
      "all branching entropies was computed # words = 361598\n",
      "all accessor variety was computed # words = 361598\n"
     ]
    }
   ],
   "source": [
    "### Sonlpy í•™ìŠµ ì§„í–‰\n",
    "word_extractor = WordExtractor()\n",
    "# í•™ìŠµ ì§„í–‰í•˜ì—¬ ë‹¨ì–´ë³„ ì ìˆ˜\n",
    "word_extractor.train(corpus)\n",
    "# ë‹¨ì–´ë³„ ì ìˆ˜í‘œ ì¶”ì¶œ\n",
    "word_score_table = word_extractor.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] - ë \n",
      "[1] - ì‹¶\n",
      "[2] - ë»˜\n",
      "[3] - 5\n",
      "[4] - ë‰´\n",
      "[5] - ì—­\n",
      "[6] - ë´\n",
      "[7] - ê¶\n",
      "[8] - ê· \n",
      "[9] - ë‚±\n",
      "[10] - ë\n",
      "[11] - ì°”\n",
      "[12] - ì½©\n",
      "[13] - 7\n",
      "[14] - íŒ°\n",
      "[15] - ìœ½\n",
      "[16] - ëŠ¥\n",
      "[17] - ì»·\n",
      "[18] - í†ˆ\n",
      "[19] - í´\n",
      "[20] - ë±€\n",
      "[21] - ìˆ\n",
      "[22] - ë¹•\n",
      "[23] - í–¥\n",
      "[24] - í•\n",
      "[25] - ì ‘\n",
      "[26] - ã…‡\n",
      "[27] - ë“\n",
      "[28] - ëˆ„\n",
      "[29] - ê·¸\n",
      "[30] - ì ˆ\n"
     ]
    }
   ],
   "source": [
    "# ë‹¨ì–´ë³„ ì ìˆ˜í‘œ í™•ì¸\n",
    "for idx, key in enumerate(iterable = word_score_table.keys()):\n",
    "    print(f'[{idx}] - {key}')\n",
    "    if idx==30: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‘ì§‘í™•ë¥ (cohesion probability) : ë‚´ë¶€ ë¬¸ìì—´(substring)ì´ ì–¼ë§ˆë‚˜ ì‘ì§‘í•˜ì—¬ ìì£¼ ë“±ì¥í•˜ëŠ”ì§€ë¥¼ íŒë‹¨í•˜ëŠ” ì²™ë„\n",
    "\n",
    "#    - ì›ë¦¬ : ë¬¸ìì—´ì„ ë¬¸ì ë‹¨ìœ„ë¡œ ë¶„ë¦¬, ì™¼ìª½ë¶€í„° ìˆœì„œëŒ€ë¡œ ë¬¸ìë¥¼ ì¶”ê°€\n",
    "#             ê° ë¬¸ìì—´ì´ ì£¼ì–´ì¡Œì„ ë•Œ ê·¸ ë‹¤ìŒ ë¬¸ìê°€ ë‚˜ì˜¬ í™•ë¥ ì„ ê³„ì‚°/ ëˆ„ì ê³± í•œ ê°’\n",
    "\n",
    "#    - ê°’ì´ ë†’ì„ìˆ˜ë¡ : ì „ì²´ ì½”í¼ìŠ¤ì—ì„œ ì´ ë¬¸ìì—´ ì‹œí€¸ìŠ¤ëŠ” í•˜ë‚˜ì˜ ë‹¨ì–´ë¡œ ë“±ì¥í•  ê°€ëŠ¥ì„± ë†’ìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06393648140409527"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_score_table['ë°”ë‹¤'].cohesion_forward # ì¼ë°˜ ëª…ì‚¬ë§Œ ë‚˜ì˜¬ í™•ë¥ ì„ ë‚®ë‹¤ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11518621707955429"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_score_table['ë°”ë‹¤ì—'].cohesion_forward # \"ë°”ë‹¤ì—\"ì²˜ëŸ¼ ëª…ì‚¬ + ì¡°ì‚¬ê°€ ë‚˜ì˜¬ í™•ë¥ ì„ ìƒëŒ€ì ìœ¼ë¡œ ë†’ë‹¤ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_score_table['ë°”'].cohesion_forward # ì–˜ë§Œ ë‚˜ì˜¬ í™•ë¥ ì€ 0ì´ë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOYNLPì˜ L tokenizer\n",
    "# - ë„ì–´ì“°ê¸° ë‹¨ìœ„ë¡œ ë‚˜ëˆˆ ì–´ì ˆ í† í° : Lí† í° + Rí† í°\n",
    "# (ì˜ˆ : 'ê³µì›ì—' -> 'ê³µì›' + 'ì—',  'ê³µë¶€í•˜ëŠ”' -> 'ê³µë¶€' + 'í•˜ëŠ”')\n",
    "# ë¶„ë¦¬ ê¸°ì¤€ : ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ Lí† í°ì„ ì°¾ì•„ë‚´ëŠ” ì›ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('êµ­ì œì‚¬íšŒ', 'ì™€'), ('ìš°ë¦¬', 'ì˜'), ('ë…¸ë ¥', 'ë“¤ë¡œ'), ('ë²”ì£„', 'ë¥¼'), ('ì²™ê²°', 'í•˜ì')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from soynlp.tokenizer import LTokenizer\n",
    "\n",
    "scores = {word:score.cohesion_forward for word, score in word_score_table.items()} # ì‘ì§‘í™•ë¥ ì´ ë†’ê²Œ ë˜ë„ë¡ ë½‘ì•„ë‚¸ë‹¤ \n",
    "l_tokenizer = LTokenizer(scores=scores) # ê·¸ëƒ¥ ìª¼ê°œì§€ ë§ê³ , LTokenizerë¡œ ìª¼ê°œì! \n",
    "l_tokenizer.tokenize(\"êµ­ì œì‚¬íšŒì™€ ìš°ë¦¬ì˜ ë…¸ë ¥ë“¤ë¡œ ë²”ì£„ë¥¼ ì²™ê²°í•˜ì\", flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['êµ­ì œì‚¬íšŒ', 'ì™€', 'ìš°ë¦¬', 'ì˜', 'ë…¸ë ¥', 'ë“¤ë¡œ', 'ë²”ì£„', 'ë¥¼', 'ì²™ê²°', 'í•˜ì']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ìµœëŒ€ ì ìˆ˜ í† í¬ë‚˜ì´ì €\n",
    "# ë„ì–´ì“°ê¸°ê°€ ë˜ì§€ ì•ŠëŠ” ë¬¸ì¥ì—ì„œ ì ìˆ˜ê°€ ë†’ì€ ê¸€ì ì‹œí€¸ìŠ¤ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì°¾ì•„ë‚´ëŠ” í† í¬ë‚˜ì´ì €\n",
    "# ë„ì–´ì“°ê¸°ê°€ ë˜ì–´ ìˆì§€ ì•Šì€ ë¬¸ì¥ì„ ë„£ì–´ì„œ ì ìˆ˜ë¥¼ í†µí•´ í† í°í™” ëœ ê²°ê³¼\n",
    "from soynlp.tokenizer import MaxScoreTokenizer\n",
    "\n",
    "maxscore_tokenizer = MaxScoreTokenizer(scores=scores)\n",
    "maxscore_tokenizer.tokenize(\"êµ­ì œì‚¬íšŒì™€ìš°ë¦¬ì˜ë…¸ë ¥ë“¤ë¡œë²”ì£„ë¥¼ì²™ê²°í•˜ì\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " 0.07856876882976202,\n",
       " 0.09217735975351507,\n",
       " 0.20075093164820865,\n",
       " 0.17387399904982392)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_score_table['êµ­'].cohesion_forward ,word_score_table['êµ­ì œ'].cohesion_forward ,word_score_table['êµ­ì œì‚¬'].cohesion_forward ,word_score_table['êµ­ì œì‚¬íšŒ'].cohesion_forward ,word_score_table['êµ­ì œì‚¬íšŒì™€'].cohesion_forward # \"ì™€\"ê°€ ìƒê¸°ìë§ˆì í™•ë¥ ì´ ë–¨ì–´ì§€ë¯€ë¡œ, ì—¬ê¸°ì„œ ëŠì–´ì£¼ëŠ” ê²ƒì´ë‹¤ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì•„ã…‹ì˜í™”ì¡´ì¼ì“° ã… \n",
      "ì•„ã…‹ã…‹ì˜í™”ì¡´ì¼ì“°ã… ã… \n",
      "ì•„ã…‹ã…‹ã…‹ì˜í™”ì¡´ì¼ì“° ã… ã… ã… ã„´ã„´ã„´ğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "# SOYNLPë¥¼ ì´ìš©í•œ ë°˜ë³µë˜ëŠ” ë¬¸ì ì •ì œ\n",
    "# ã…‹ã…‹, ã…ã… ë“±ì˜ ì´ëª¨í‹°ì½˜ì¸ ê²½ìš° ë¶ˆí•„ìš”í•˜ê²Œ ì—°ì†ë˜ëŠ” ê²½ìš° ë§ìŒ\n",
    "# ã…‹ã…‹, ã…‹ã…‹ã…‹, ã…‹ã…‹ã…‹ã…‹ì™€ ê°™ì€ ê²½ìš°ë¥¼ ëª¨ë‘ ì„œë¡œ ë‹¤ë¥¸ ë‹¨ì–´ë¡œ ì²˜ë¦¬í•˜ëŠ” ê²ƒì€ ë¶ˆí•„ìš”\n",
    "# >> ë°˜ë³µë˜ëŠ” ê²ƒì€ í•˜ë‚˜ë¡œ ì •ê·œí™” \n",
    "\n",
    "from soynlp.normalizer import *\n",
    "\n",
    "print(emoticon_normalize(\"ì•œã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ì´ì˜í™”ì¡´ì¼ì“° ã… ã… ã… ã… \", num_repeats=1))\n",
    "print(emoticon_normalize(\"ì•œã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ì´ì˜í™”ì¡´ì¼ì“°ã… ã… ã… ã… ã… ã… ã… ã… \", num_repeats=2))\n",
    "print(emoticon_normalize(\"ì•œã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ì´ì˜í™”ì¡´ì¼ì“° ã… ã… ã… ã… ã… ã… ã… ã„´ã„´ã„´ã„´ã„´ã„´ğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜Š\", num_repeats=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì™€í•˜í•˜í•«\n"
     ]
    }
   ],
   "source": [
    "print(repeat_normalize(\"ì™€í•˜í•˜í•˜í•˜í•«\", num_repeats=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install customized_konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\NLP\\lib\\site-packages\\konlpy\\tag\\_okt.py:17: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ì€', 'ê²½ì´', 'ëŠ”', 'ì‚¬ë¬´ì‹¤', 'ë¡œ', 'ê°”ìŠµë‹ˆë‹¤', '.']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ckonlpy.tag import Twitter\n",
    "\n",
    "twitter = Twitter()\n",
    "twitter.morphs('ì€ê²½ì´ëŠ” ì‚¬ë¬´ì‹¤ë¡œ ê°”ìŠµë‹ˆë‹¤.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter.add_dictionary(words=\"ì€ê²½ì´\", tag=\"Noun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ì€ê²½ì´', 'ëŠ”', 'ì‚¬ë¬´ì‹¤', 'ë¡œ', 'ê°”ìŠµë‹ˆë‹¤', '.']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter.morphs('ì€ê²½ì´ëŠ” ì‚¬ë¬´ì‹¤ë¡œ ê°”ìŠµë‹ˆë‹¤.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë§¥ìº¡ì€ ì½”ë©ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‚¬ìš©ìë‹¨ì–´ ì‚¬ìš©ìí’ˆì‚¬\n",
      "ì¶”ê°€í• ë‹¨ì–´ ì¶”ê°€í• í’ˆì‚¬\n",
      "['ì‚¬ìš©ì', 'ë‹¨ì–´', 'ë¥¼', 'ì¶”ê°€', 'í•˜', 'ã„¹', 'ìˆ˜', 'ìˆ', 'ìŠµë‹ˆë‹¤', '.']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Komoran\n",
    "\n",
    "komoran = Komoran()\n",
    "\n",
    "# ì‚¬ìš©ì ì •ì˜ ì‚¬ì „ ì¶”ê°€\n",
    "# ì¶”ê°€í•  ë‹¨ì–´ì™€ ê·¸ì— í•´ë‹¹í•˜ëŠ” í’ˆì‚¬ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë„£ì–´ì¤ë‹ˆë‹¤.\n",
    "user_dictionary = [('ì‚¬ìš©ìë‹¨ì–´', 'ì‚¬ìš©ìí’ˆì‚¬'), ('ì¶”ê°€í• ë‹¨ì–´', 'ì¶”ê°€í• í’ˆì‚¬')]\n",
    "\n",
    "# ì‚¬ìš©ì ì •ì˜ ì‚¬ì „ ì ìš©\n",
    "for word, pos in user_dictionary:\n",
    "    print(word, pos)\n",
    "    komoran.tagset.setdefault(word, pos)\n",
    "\n",
    "# ì˜ˆì‹œ ë¬¸ì¥ í˜•íƒœì†Œ ë¶„ì„\n",
    "result = komoran.morphs(\"ì‚¬ìš©ìë‹¨ì–´ë¥¼ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EC': 'ì—°ê²° ì–´ë¯¸',\n",
       " 'EF': 'ì¢…ê²° ì–´ë¯¸',\n",
       " 'EP': 'ì„ ì–´ë§ì–´ë¯¸',\n",
       " 'ETM': 'ê´€í˜•í˜• ì „ì„± ì–´ë¯¸',\n",
       " 'ETN': 'ëª…ì‚¬í˜• ì „ì„± ì–´ë¯¸',\n",
       " 'IC': 'ê°íƒ„ì‚¬',\n",
       " 'JC': 'ì ‘ì† ì¡°ì‚¬',\n",
       " 'JKB': 'ë¶€ì‚¬ê²© ì¡°ì‚¬',\n",
       " 'JKC': 'ë³´ê²© ì¡°ì‚¬',\n",
       " 'JKG': 'ê´€í˜•ê²© ì¡°ì‚¬',\n",
       " 'JKO': 'ëª©ì ê²© ì¡°ì‚¬',\n",
       " 'JKQ': 'ì¸ìš©ê²© ì¡°ì‚¬',\n",
       " 'JKS': 'ì£¼ê²© ì¡°ì‚¬',\n",
       " 'JKV': 'í˜¸ê²© ì¡°ì‚¬',\n",
       " 'JX': 'ë³´ì¡°ì‚¬',\n",
       " 'MAG': 'ì¼ë°˜ ë¶€ì‚¬',\n",
       " 'MAJ': 'ì ‘ì† ë¶€ì‚¬',\n",
       " 'MM': 'ê´€í˜•ì‚¬',\n",
       " 'NA': 'ë¶„ì„ë¶ˆëŠ¥ë²”ì£¼',\n",
       " 'NF': 'ëª…ì‚¬ì¶”ì •ë²”ì£¼',\n",
       " 'NNB': 'ì˜ì¡´ ëª…ì‚¬',\n",
       " 'NNG': 'ì¼ë°˜ ëª…ì‚¬',\n",
       " 'NNP': 'ê³ ìœ  ëª…ì‚¬',\n",
       " 'NP': 'ëŒ€ëª…ì‚¬',\n",
       " 'NR': 'ìˆ˜ì‚¬',\n",
       " 'NV': 'ìš©ì–¸ì¶”ì •ë²”ì£¼',\n",
       " 'SE': 'ì¤„ì„í‘œ',\n",
       " 'SF': 'ë§ˆì¹¨í‘œ, ë¬¼ìŒí‘œ, ëŠë‚Œí‘œ',\n",
       " 'SH': 'í•œì',\n",
       " 'SL': 'ì™¸êµ­ì–´',\n",
       " 'SN': 'ìˆ«ì',\n",
       " 'SO': 'ë¶™ì„í‘œ(ë¬¼ê²°,ìˆ¨ê¹€,ë¹ ì§)',\n",
       " 'SP': 'ì‰¼í‘œ,ê°€ìš´ëƒì ,ì½œë¡ ,ë¹—ê¸ˆ',\n",
       " 'SS': 'ë”°ì˜´í‘œ,ê´„í˜¸í‘œ,ì¤„í‘œ',\n",
       " 'SW': 'ê¸°íƒ€ê¸°í˜¸ (ë…¼ë¦¬ìˆ˜í•™ê¸°í˜¸,í™”íê¸°í˜¸)',\n",
       " 'VA': 'í˜•ìš©ì‚¬',\n",
       " 'VCN': 'ë¶€ì • ì§€ì •ì‚¬',\n",
       " 'VCP': 'ê¸ì • ì§€ì •ì‚¬',\n",
       " 'VV': 'ë™ì‚¬',\n",
       " 'VX': 'ë³´ì¡° ìš©ì–¸',\n",
       " 'XPN': 'ì²´ì–¸ ì ‘ë‘ì‚¬',\n",
       " 'XR': 'ì–´ê·¼',\n",
       " 'XSA': 'í˜•ìš©ì‚¬ íŒŒìƒ ì ‘ë¯¸ì‚¬',\n",
       " 'XSN': 'ëª…ì‚¬íŒŒìƒ ì ‘ë¯¸ì‚¬',\n",
       " 'XSV': 'ë™ì‚¬ íŒŒìƒ ì ‘ë¯¸ì‚¬',\n",
       " 'ì‚¬ìš©ìë‹¨ì–´': 'ì‚¬ìš©ìí’ˆì‚¬',\n",
       " 'ì¶”ê°€í• ë‹¨ì–´': 'ì¶”ê°€í• í’ˆì‚¬'}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "komoran.tagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
